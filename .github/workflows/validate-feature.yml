name: Validate Feature Guide

on:
  push:
    paths:
      - '**/validation/validation-payload.json'
      - '**/validation/journeys-payload.json'
  workflow_dispatch:
    inputs:
      feature_folder:
        description: 'Feature folder to validate (e.g., air-ticket/v19)'
        required: true
      debug_mode:
        description: 'Enable debug logging'
        required: false
        default: 'false'
      max_tests_per_chunk:
        description: 'Maximum tests per chunk (default: 5)'
        required: false
        default: '5'

env:
  PLAYWRIGHT_MCP_VERSION: "0.0.42"
  NODE_VERSION: "20"
  MAX_TESTS_PER_CHUNK: ${{ github.event.inputs.max_tests_per_chunk || '5' }}

jobs:
  # ============================================
  # JOB 1: PREPARE - Detect and split payload into chunks
  # ============================================
  prepare:
    runs-on: [self-hosted, validation]
    timeout-minutes: 5

    outputs:
      skip: ${{ steps.detect.outputs.skip }}
      folder: ${{ steps.detect.outputs.folder }}
      payload_file: ${{ steps.detect.outputs.payload_file }}
      payload_format: ${{ steps.detect.outputs.payload_format }}
      result_file: ${{ steps.detect.outputs.result_file }}
      report_file: ${{ steps.detect.outputs.report_file }}
      screenshots_dir: ${{ steps.detect.outputs.screenshots_dir }}
      log_file: ${{ steps.detect.outputs.log_file }}
      feature_name: ${{ steps.parse_payload.outputs.feature_name }}
      feature_slug: ${{ steps.parse_payload.outputs.feature_slug }}
      version: ${{ steps.parse_payload.outputs.version }}
      total_tests: ${{ steps.parse_payload.outputs.total_tests }}
      must_test: ${{ steps.parse_payload.outputs.must_test }}
      should_test: ${{ steps.parse_payload.outputs.should_test }}
      chunk_count: ${{ steps.split.outputs.chunk_count }}
      chunk_matrix: ${{ steps.split.outputs.chunk_matrix }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        timeout-minutes: 3
        with:
          fetch-depth: 2
          persist-credentials: false

      - name: Detect validation request
        id: detect
        run: |
          if [ -n "${{ github.event.inputs.feature_folder }}" ]; then
            FOLDER="${{ github.event.inputs.feature_folder }}"
          else
            CHANGED_FILE=$(git diff --name-only HEAD~1 HEAD | grep -E 'validation/(validation-payload|journeys-payload)\.json' | head -1)
            if [ -n "$CHANGED_FILE" ]; then
              FOLDER=$(echo "$CHANGED_FILE" | sed 's|/validation/.*||')
              echo "Detected changed file: $CHANGED_FILE"
              echo "Extracted folder: $FOLDER"
            fi
          fi

          if [ -z "$FOLDER" ]; then
            echo "No validation request found"
            echo "skip=true" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Find payload file
          if [ -f "$GITHUB_WORKSPACE/$FOLDER/validation/journeys-payload.json" ]; then
            PAYLOAD_FILE="$FOLDER/validation/journeys-payload.json"
            PAYLOAD_FORMAT="journeys-v1"
          elif [ -f "$GITHUB_WORKSPACE/$FOLDER/validation/validation-payload.json" ]; then
            PAYLOAD_FILE="$FOLDER/validation/validation-payload.json"
            PAYLOAD_FORMAT="journeys-v1"
          else
            echo "No validation payload found in $FOLDER/validation/"
            echo "skip=true" >> $GITHUB_OUTPUT
            exit 0
          fi

          echo "folder=$FOLDER" >> $GITHUB_OUTPUT
          echo "payload_file=$PAYLOAD_FILE" >> $GITHUB_OUTPUT
          echo "payload_format=$PAYLOAD_FORMAT" >> $GITHUB_OUTPUT
          echo "result_file=$FOLDER/validation/result.json" >> $GITHUB_OUTPUT
          echo "report_file=$FOLDER/validation/report.md" >> $GITHUB_OUTPUT
          echo "screenshots_dir=$FOLDER/validation/screenshots" >> $GITHUB_OUTPUT
          echo "log_file=$FOLDER/validation/validation.log" >> $GITHUB_OUTPUT
          echo "skip=false" >> $GITHUB_OUTPUT
          echo "Found validation request: $FOLDER (format: $PAYLOAD_FORMAT)"

      - name: Parse and validate payload
        id: parse_payload
        if: steps.detect.outputs.skip != 'true'
        run: |
          PAYLOAD_PATH="$GITHUB_WORKSPACE/${{ steps.detect.outputs.payload_file }}"

          echo "Parsing payload: $PAYLOAD_PATH"

          # Validate JSON
          if ! jq empty "$PAYLOAD_PATH" 2>/dev/null; then
            echo "Invalid JSON in payload file"
            exit 1
          fi

          # Extract metadata (handle array or object)
          IS_ARRAY=$(jq 'if type == "array" then true else false end' "$PAYLOAD_PATH")

          if [ "$IS_ARRAY" = "true" ]; then
            FEATURE_NAME=$(jq -r '.[0].metadata.feature_name // "unknown"' "$PAYLOAD_PATH")
            FEATURE_SLUG=$(jq -r '.[0].metadata.feature_slug // "unknown"' "$PAYLOAD_PATH")
            VERSION=$(jq -r '.[0].metadata.version // "v1"' "$PAYLOAD_PATH")
            TOTAL_TESTS=$(jq -r '.[0].summary.total_tests // 0' "$PAYLOAD_PATH")
            MUST_TEST=$(jq -r '.[0].summary.by_priority.must_test // 0' "$PAYLOAD_PATH")
            SHOULD_TEST=$(jq -r '.[0].summary.by_priority.should_test // 0' "$PAYLOAD_PATH")
          else
            FEATURE_NAME=$(jq -r '.metadata.feature_name // "unknown"' "$PAYLOAD_PATH")
            FEATURE_SLUG=$(jq -r '.metadata.feature_slug // "unknown"' "$PAYLOAD_PATH")
            VERSION=$(jq -r '.metadata.version // "v1"' "$PAYLOAD_PATH")
            TOTAL_TESTS=$(jq -r '.summary.total_tests // 0' "$PAYLOAD_PATH")
            MUST_TEST=$(jq -r '.summary.by_priority.must_test // 0' "$PAYLOAD_PATH")
            SHOULD_TEST=$(jq -r '.summary.by_priority.should_test // 0' "$PAYLOAD_PATH")
          fi

          echo "feature_name=$FEATURE_NAME" >> $GITHUB_OUTPUT
          echo "feature_slug=$FEATURE_SLUG" >> $GITHUB_OUTPUT
          echo "version=$VERSION" >> $GITHUB_OUTPUT
          echo "total_tests=$TOTAL_TESTS" >> $GITHUB_OUTPUT
          echo "must_test=$MUST_TEST" >> $GITHUB_OUTPUT
          echo "should_test=$SHOULD_TEST" >> $GITHUB_OUTPUT

          echo "============================================"
          echo "Payload Summary:"
          echo "  Feature: $FEATURE_NAME ($FEATURE_SLUG)"
          echo "  Version: $VERSION"
          echo "  Total Tests: $TOTAL_TESTS"
          echo "  Must Test: $MUST_TEST"
          echo "  Should Test: $SHOULD_TEST"
          echo "============================================"

      - name: Split payload into chunks
        id: split
        if: steps.detect.outputs.skip != 'true'
        run: |
          PAYLOAD_PATH="$GITHUB_WORKSPACE/${{ steps.detect.outputs.payload_file }}"
          FOLDER="${{ steps.detect.outputs.folder }}"
          CHUNKS_DIR="$GITHUB_WORKSPACE/$FOLDER/validation/chunks"
          MAX_PER_CHUNK="${{ env.MAX_TESTS_PER_CHUNK }}"

          # Clean up any previous chunks
          rm -rf "$CHUNKS_DIR"
          mkdir -p "$CHUNKS_DIR"

          # Check if payload is array or object and get total test count
          IS_ARRAY=$(jq 'if type == "array" then true else false end' "$PAYLOAD_PATH")

          if [ "$IS_ARRAY" = "true" ]; then
            TOTAL_TESTS=$(jq -r '.[0].tests | length' "$PAYLOAD_PATH")
            JQ_ROOT=".[0]"
          else
            TOTAL_TESTS=$(jq -r '.tests | length' "$PAYLOAD_PATH")
            JQ_ROOT=""
          fi

          if [ "$TOTAL_TESTS" -eq 0 ]; then
            echo "No tests found in payload"
            echo "chunk_count=0" >> $GITHUB_OUTPUT
            echo "chunk_matrix=[]" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Calculate number of chunks needed
          CHUNK_COUNT=$(( (TOTAL_TESTS + MAX_PER_CHUNK - 1) / MAX_PER_CHUNK ))

          echo "============================================"
          echo "Chunking Strategy:"
          echo "  Total Tests: $TOTAL_TESTS"
          echo "  Max per Chunk: $MAX_PER_CHUNK"
          echo "  Chunk Count: $CHUNK_COUNT"
          echo "============================================"

          # Extract definitions (shared across all chunks)
          # These are small with reference-based architecture
          if [ "$IS_ARRAY" = "true" ]; then
            jq '.[0].definitions // {}' "$PAYLOAD_PATH" > "$CHUNKS_DIR/definitions.json"
            jq '{metadata: .[0].metadata, instructions: .[0].instructions}' "$PAYLOAD_PATH" > "$CHUNKS_DIR/shared.json"
          else
            jq '.definitions // {}' "$PAYLOAD_PATH" > "$CHUNKS_DIR/definitions.json"
            jq '{metadata: .metadata, instructions: .instructions}' "$PAYLOAD_PATH" > "$CHUNKS_DIR/shared.json"
          fi
          DEFS_SIZE=$(wc -c < "$CHUNKS_DIR/definitions.json")
          echo "Definitions size: $DEFS_SIZE bytes (shared)"

          # Create chunk matrix for GitHub Actions
          MATRIX_JSON="["

          for i in $(seq 0 $((CHUNK_COUNT - 1))); do
            START=$((i * MAX_PER_CHUNK))

            # Create chunk payload with:
            # - Full definitions (shared, reference-based)
            # - Full metadata and instructions
            # - Subset of tests
            if [ "$IS_ARRAY" = "true" ]; then
              jq --argjson start $START --argjson count $MAX_PER_CHUNK --argjson idx $i --argjson chunks $CHUNK_COUNT --argjson total $TOTAL_TESTS '
                {
                  metadata: .[0].metadata,
                  instructions: .[0].instructions,
                  definitions: .[0].definitions,
                  tests: (.[0].tests[$start:$start+$count]),
                  chunk_info: {
                    chunk_index: $idx,
                    chunk_count: $chunks,
                    tests_in_chunk: (.[0].tests[$start:$start+$count] | length),
                    total_tests: $total
                  }
                }
              ' "$PAYLOAD_PATH" > "$CHUNKS_DIR/chunk-$i.json"
            else
              jq --argjson start $START --argjson count $MAX_PER_CHUNK --argjson idx $i --argjson chunks $CHUNK_COUNT --argjson total $TOTAL_TESTS '
                {
                  metadata: .metadata,
                  instructions: .instructions,
                  definitions: .definitions,
                  tests: (.tests[$start:$start+$count]),
                  chunk_info: {
                    chunk_index: $idx,
                    chunk_count: $chunks,
                    tests_in_chunk: (.tests[$start:$start+$count] | length),
                    total_tests: $total
                  }
                }
              ' "$PAYLOAD_PATH" > "$CHUNKS_DIR/chunk-$i.json"
            fi

            CHUNK_SIZE=$(wc -c < "$CHUNKS_DIR/chunk-$i.json")
            TESTS_IN_CHUNK=$(jq '.tests | length' "$CHUNKS_DIR/chunk-$i.json")
            echo "  Chunk $i: $TESTS_IN_CHUNK tests, $CHUNK_SIZE bytes"

            # Build matrix JSON
            if [ $i -gt 0 ]; then
              MATRIX_JSON="${MATRIX_JSON},"
            fi
            MATRIX_JSON="${MATRIX_JSON}{\"index\":$i}"
          done

          MATRIX_JSON="${MATRIX_JSON}]"

          echo "chunk_count=$CHUNK_COUNT" >> $GITHUB_OUTPUT
          echo "chunk_matrix=$MATRIX_JSON" >> $GITHUB_OUTPUT

          echo "============================================"
          echo "Chunks created: $CHUNK_COUNT"
          echo "Matrix: $MATRIX_JSON"
          echo "============================================"

      - name: Pre-flight Health Checks
        id: preflight
        if: steps.detect.outputs.skip != 'true'
        run: |
          echo "Running pre-flight checks..."
          ERRORS=""

          if [ -z "${{ secrets.ANTHROPIC_API_KEY }}" ]; then
            ERRORS="${ERRORS}ANTHROPIC_API_KEY not configured\n"
          else
            echo "ANTHROPIC_API_KEY configured"
          fi

          if [ -z "${{ secrets.APP_BASE_URL }}" ]; then
            ERRORS="${ERRORS}APP_BASE_URL not configured\n"
          else
            echo "APP_BASE_URL configured"
          fi

          AVAILABLE_GB=$(df -g "$GITHUB_WORKSPACE" | tail -1 | awk '{print $4}')
          if [ "$AVAILABLE_GB" -lt 2 ]; then
            ERRORS="${ERRORS}Low disk space: ${AVAILABLE_GB}GB available (need 2GB)\n"
          else
            echo "Disk space OK: ${AVAILABLE_GB}GB available"
          fi

          if [ -n "$ERRORS" ]; then
            echo "Pre-flight checks failed:"
            echo -e "$ERRORS"
            exit 1
          fi

          echo "All pre-flight checks passed"

  # ============================================
  # JOB 2: VALIDATE - Run validation for each chunk (parallel)
  # ============================================
  validate:
    needs: prepare
    if: needs.prepare.outputs.skip != 'true' && needs.prepare.outputs.chunk_count != '0'
    runs-on: [self-hosted, validation]
    timeout-minutes: 20  # Reduced timeout per chunk (was 45 for full payload)

    strategy:
      fail-fast: false  # Continue other chunks if one fails
      max-parallel: 2   # Limit parallel runs to avoid resource contention
      matrix:
        chunk: ${{ fromJson(needs.prepare.outputs.chunk_matrix) }}

    outputs:
      chunk_status: ${{ steps.validate_outputs.outputs.status }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        timeout-minutes: 3
        with:
          fetch-depth: 1
          persist-credentials: false

      - name: Prepare Chunk Directories
        run: |
          FOLDER="${{ needs.prepare.outputs.folder }}"
          CHUNK_INDEX="${{ matrix.chunk.index }}"

          # Create chunk-specific output directories
          mkdir -p "$GITHUB_WORKSPACE/$FOLDER/validation/chunks/results"
          mkdir -p "$GITHUB_WORKSPACE/$FOLDER/validation/screenshots/chunk-$CHUNK_INDEX"

          echo "Chunk $CHUNK_INDEX directories ready"

      - name: Install Claude CLI
        run: |
          echo "Installing Claude CLI..."
          npm install -g @anthropic-ai/claude-code
          export PATH="$(npm prefix -g)/bin:$PATH"

          if ! command -v claude &> /dev/null; then
            echo "Claude CLI installation failed"
            exit 1
          fi

          claude --version
          echo "Claude CLI installed"

      - name: Install Playwright MCP and Browsers
        run: |
          echo "Installing Playwright MCP v${PLAYWRIGHT_MCP_VERSION}..."
          npm install -g @playwright/mcp@${PLAYWRIGHT_MCP_VERSION}

          echo "Verifying Chrome browser installation..."
          # Check multiple possible locations
          if [ -d ~/.cache/ms-playwright/chrome* ] || [ -d ~/.cache/ms-playwright/chromium* ]; then
            echo "Browser found in user cache"
          elif [ -d /root/.cache/ms-playwright/chrome* ] || [ -d /root/.cache/ms-playwright/chromium* ]; then
            echo "Browser found in root cache"
          elif command -v google-chrome &> /dev/null || command -v chromium &> /dev/null; then
            echo "System Chrome/Chromium found"
          else
            echo "Installing Chrome browser (without system deps)..."
            # Use --no-shell to avoid interactive prompts, skip system deps
            PLAYWRIGHT_SKIP_BROWSER_DOWNLOAD=0 npx -y playwright install chromium --no-shell 2>/dev/null || \
            npx -y playwright install chromium 2>/dev/null || \
            echo "Warning: Browser install may have issues, will try to proceed"
          fi

          echo "Playwright MCP v${PLAYWRIGHT_MCP_VERSION} ready"

      - name: Setup MCP Configuration
        env:
          PLAYWRIGHT_MCP_VERSION: ${{ env.PLAYWRIGHT_MCP_VERSION }}
        run: |
          FOLDER="${{ needs.prepare.outputs.folder }}"
          CHUNK_INDEX="${{ matrix.chunk.index }}"
          SCREENSHOT_PATH="$GITHUB_WORKSPACE/$FOLDER/validation/screenshots/chunk-$CHUNK_INDEX"

          echo "Setting up MCP configuration..."
          echo "  Screenshot path: $SCREENSHOT_PATH"
          echo "  Playwright MCP version: $PLAYWRIGHT_MCP_VERSION"

          rm -f "$GITHUB_WORKSPACE/.mcp.json"
          mkdir -p "$SCREENSHOT_PATH"

          # Create MCP config without heredoc indentation issues
          echo '{"mcpServers":{"playwright":{"command":"npx","args":["-y","@playwright/mcp@'"$PLAYWRIGHT_MCP_VERSION"'","--output-dir","'"$SCREENSHOT_PATH"'","--browser","chromium","--timeout","300000"]}}}' > "$GITHUB_WORKSPACE/.mcp.json"

          # Validate JSON
          echo "Validating MCP config..."
          if jq empty "$GITHUB_WORKSPACE/.mcp.json" 2>/dev/null; then
            echo "MCP config is valid JSON"
            jq . "$GITHUB_WORKSPACE/.mcp.json"
          else
            echo "ERROR: Invalid MCP config JSON!"
            cat "$GITHUB_WORKSPACE/.mcp.json"
            exit 1
          fi

          # Test MCP server startup
          echo "Testing Playwright MCP server..."
          timeout 10 npx -y "@playwright/mcp@$PLAYWRIGHT_MCP_VERSION" --help > /dev/null 2>&1 && echo "Playwright MCP responds" || echo "WARNING: Playwright MCP test failed"

          echo "MCP config for chunk $CHUNK_INDEX ready"

      - name: Generate Chunk Validation Prompt
        id: generate_prompt
        env:
          APP_BASE_URL: ${{ secrets.APP_BASE_URL }}
          APP_USERNAME: ${{ secrets.APP_USERNAME }}
          APP_PASSWORD: ${{ secrets.APP_PASSWORD }}
          FOLDER: ${{ needs.prepare.outputs.folder }}
          FEATURE_NAME: ${{ needs.prepare.outputs.feature_name }}
          CHUNK_INDEX: ${{ matrix.chunk.index }}
          CHUNK_COUNT: ${{ needs.prepare.outputs.chunk_count }}
        run: |
          PROMPT_FILE="$GITHUB_WORKSPACE/$FOLDER/validation/chunks/.prompt-chunk-$CHUNK_INDEX.txt"
          CHUNK_PAYLOAD="$GITHUB_WORKSPACE/$FOLDER/validation/chunks/chunk-$CHUNK_INDEX.json"
          CHUNK_RESULT="$GITHUB_WORKSPACE/$FOLDER/validation/chunks/results/result-chunk-$CHUNK_INDEX.json"
          SCREENSHOT_DIR="$GITHUB_WORKSPACE/$FOLDER/validation/screenshots/chunk-$CHUNK_INDEX"

          cat > "$PROMPT_FILE" << 'PROMPT_HEADER'
          # UI Validation Agent - Journey-Based Testing (Chunked Execution)

          You are a UI validation agent that executes test cases against a live web application.
          Your task is to validate the UI based on the provided CHUNK of tests.

          ## CHUNK EXECUTION MODE
          This is a CHUNKED validation run. You are processing a SUBSET of tests.
          - Complete all tests in YOUR chunk only
          - Other chunks are running in parallel
          - Results will be merged after all chunks complete

          ## CRITICAL INSTRUCTIONS
          1. You MUST complete all tests in this chunk
          2. You MUST capture screenshots for each major step (prefix with chunk index)
          3. You MUST write chunk result JSON before finishing
          4. You MUST handle login first before any validation
          5. Focus on tests in order - they are pre-sorted by priority

          ═══════════════════════════════════════════════════════════════
          ## CRITICAL: REFERENCE-BASED PAYLOAD STRUCTURE
          ═══════════════════════════════════════════════════════════════

          This payload uses ID REFERENCES to avoid duplication. You MUST resolve
          references before executing any test. The payload contains an "instructions"
          section that also explains this - read it carefully.

          ### REFERENCE RESOLUTION TABLE
          ┌─────────────────────┬──────────────────────────────────────────────┐
          │ Field               │ How to Resolve                               │
          ├─────────────────────┼──────────────────────────────────────────────┤
          │ access_path_ref     │ Look up in definitions.access_paths          │
          │                     │ (configuration OR operational) by path_id    │
          ├─────────────────────┼──────────────────────────────────────────────┤
          │ setup_flow_ref      │ Look up in definitions.setup_flows           │
          │                     │ by flow_id → get steps array                 │
          ├─────────────────────┼──────────────────────────────────────────────┤
          │ business_rule_refs  │ Look up each ID in definitions.business_rules│
          │                     │ by rule_id → get rule + ui_manifestation     │
          ├─────────────────────┼──────────────────────────────────────────────┤
          │ edge_case_refs      │ Look up each ID in definitions.edge_cases    │
          │                     │ by case_id → get scenario + expected_behavior│
          ├─────────────────────┼──────────────────────────────────────────────┤
          │ ui_element_refs     │ Look up each in definitions.ui_elements      │
          │                     │ by element_name → get type + location        │
          └─────────────────────┴──────────────────────────────────────────────┘

          ### RESOLUTION EXAMPLE
          Given test with: setup_flow_ref: "SETUP-002"
          Action: Find definitions.setup_flows.find(f => f.flow_id === "SETUP-002")
          Result: Execute all steps from that flow object

          ═══════════════════════════════════════════════════════════════

          PROMPT_HEADER

          cat >> "$PROMPT_FILE" << EOF

          ## CHUNK INFO
          - Chunk Index: $CHUNK_INDEX of $CHUNK_COUNT
          - This chunk's tests are in the payload below

          ## AUTHENTICATION
          - Base URL: $APP_BASE_URL
          - Username: $APP_USERNAME
          - Password: $APP_PASSWORD

          ## OUTPUT PATHS (CHUNK-SPECIFIC)
          - Screenshots: $SCREENSHOT_DIR/
          - Chunk Result JSON: $CHUNK_RESULT

          ## FEATURE: $FEATURE_NAME

          ## CHUNK VALIDATION PAYLOAD
          Read and parse the following chunk payload file:
          $CHUNK_PAYLOAD

          ## VALIDATION WORKFLOW FOR THIS CHUNK

          ### Step 1: Login
          1. Navigate to the base URL
          2. Enter credentials and login
          3. Take screenshot: "chunk-${CHUNK_INDEX}-01-login-success.png"

          ### Step 2: Resolve References and Execute Tests
          For each test in this chunk:
          1. RESOLVE access_path_ref → get navigation steps
          2. RESOLVE setup_flow_ref → get setup steps (if present)
          3. RESOLVE business_rule_refs → get rules to validate
          4. RESOLVE edge_case_refs → get edge cases to check
          5. Navigate using the resolved navigation path
          6. Execute the resolved setup_flow steps
          7. Validate each assertion in the test
          8. Check resolved business_rules are satisfied
          9. Test resolved edge_cases if applicable
          10. Take screenshots: "chunk-${CHUNK_INDEX}-<test_id>-<step>.png"

          ### Step 3: Write Chunk Results
          Write a chunk result JSON file to: $CHUNK_RESULT
          Structure:
          {
            "chunk_index": $CHUNK_INDEX,
            "chunk_count": $CHUNK_COUNT,
            "validation_status": "completed|failed",
            "feature_name": "$FEATURE_NAME",
            "timestamp": "<ISO timestamp>",
            "summary": {
              "total_tests": <tests in this chunk>,
              "passed": <number>,
              "failed": <number>,
              "skipped": <number>
            },
            "test_results": [
              {
                "test_id": "<test_id from payload>",
                "test_name": "<test name>",
                "status": "passed|failed|skipped",
                "assertions_checked": <number>,
                "assertions_passed": <number>,
                "business_rules_validated": ["<rule_ids checked>"],
                "edge_cases_tested": ["<case_ids checked>"],
                "screenshots": ["<filename>"],
                "notes": "<any observations>"
              }
            ],
            "screenshots": ["<list of screenshots for this chunk>"]
          }

          ## IMPORTANT NOTES
          - ALWAYS resolve references before executing - never skip this step
          - Prefix ALL screenshots with "chunk-${CHUNK_INDEX}-"
          - Only process tests in YOUR chunk payload
          - If a reference cannot be resolved, log it and skip that part
          - Take screenshot on any error for debugging

          EOF

          echo "Chunk $CHUNK_INDEX prompt generated ($(wc -c < "$PROMPT_FILE") bytes)"

      - name: Pre-Validation Browser Cleanup
        run: |
          echo "Cleaning up stale Playwright processes only..."
          pkill -f "mcp-server-playwright" 2>/dev/null || true
          # Only kill Playwright-spawned browsers, NOT user's personal browsers
          pkill -f "playwright.*chromium" 2>/dev/null || true
          pkill -f "ms-playwright" 2>/dev/null || true
          sleep 2
          rm -rf /tmp/playwright-* 2>/dev/null || true
          echo "Pre-validation cleanup complete"

      - name: Run Chunk Validation
        id: validation
        env:
          FOLDER: ${{ needs.prepare.outputs.folder }}
          CHUNK_INDEX: ${{ matrix.chunk.index }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          echo "============================================"
          echo "Starting validation for Chunk $CHUNK_INDEX"
          echo "Feature: ${{ needs.prepare.outputs.feature_name }}"
          echo "Start time: $(date -u +%Y-%m-%dT%H:%M:%SZ)"
          echo "============================================"

          cd "$GITHUB_WORKSPACE"
          export PATH="$(npm prefix -g)/bin:$PATH"

          # Debug: Show environment
          echo "DEBUG: Working directory: $(pwd)"
          echo "DEBUG: MCP config exists: $(test -f .mcp.json && echo YES || echo NO)"
          echo "DEBUG: MCP config content:"
          cat .mcp.json 2>/dev/null || echo "ERROR: Cannot read .mcp.json"
          echo ""
          echo "DEBUG: Claude CLI version:"
          claude --version 2>&1 || echo "ERROR: Cannot get Claude version"
          echo ""

          PROMPT_FILE="$GITHUB_WORKSPACE/$FOLDER/validation/chunks/.prompt-chunk-$CHUNK_INDEX.txt"
          LOG_FILE="$GITHUB_WORKSPACE/$FOLDER/validation/chunks/validation-chunk-$CHUNK_INDEX.log"

          echo "DEBUG: Prompt file size: $(wc -c < "$PROMPT_FILE" 2>/dev/null || echo 0) bytes"
          echo "============================================"

          set +e
          # IMPORTANT: --print mode does NOT support MCP servers in standard way
          # We need to use expect to properly simulate an interactive TTY session
          # This allows MCP servers to initialize properly

          # Check if expect is available (pre-installed on macOS)
          if ! command -v expect &> /dev/null; then
            echo "ERROR: expect not found"
            exit 1
          fi

          # Create expect script (base64 encoded to avoid YAML parsing issues)
          # Script: FIRST handles API key dialog (send "1"), THEN waits for MCP init, THEN sends prompt
          EXPECT_B64="IyEvdXNyL2Jpbi9leHBlY3QgLWYKc2V0IHRpbWVvdXQgNjAwCnNldCBwcm9tcHRfZmlsZSBbbGluZGV4ICRhcmd2IDBdCgpzZXQgZnAgW29wZW4gJHByb21wdF9maWxlIHJdCnNldCBwcm9tcHRfY29udGVudCBbcmVhZCAkZnBdCmNsb3NlICRmcAoKc3Bhd24gY2xhdWRlIC0tbW9kZWwgc29ubmV0IC0tYWxsb3dlZFRvb2xzICJtY3BfX3BsYXl3cmlnaHRfXyosUmVhZCxXcml0ZSxHbG9iLEdyZXAiIC0tbWNwLWNvbmZpZyAiLm1jcC5qc29uIiAtLWRhbmdlcm91c2x5LXNraXAtcGVybWlzc2lvbnMgLS1tYXgtdHVybnMgMzAKCiMgRklSU1Q6IFdhaXQgZm9yIEFQSSBrZXkgY29uZmlybWF0aW9uIGRpYWxvZyB0byBhcHBlYXIKc2xlZXAgMwoKIyBTZW5kICIxIiB0byBzZWxlY3QgWWVzIGZvciB1c2luZyB0aGUgQVBJIGtleQpzZW5kIC0tICIxXHIiCgojIFdhaXQgZm9yIENsYXVkZS9NQ1Agc2VydmVycyB0byBpbml0aWFsaXplIGFmdGVyIEFQSSBrZXkgY29uZmlybWF0aW9uCnNsZWVwIDEwCgojIE5PVyBzZW5kIHRoZSBwcm9tcHQgY29udGVudApzZW5kIC0tICIkcHJvbXB0X2NvbnRlbnRcciIKCiMgV2FpdCBmb3IgQ2xhdWRlIHRvIGNvbXBsZXRlCmV4cGVjdCB7CiAgICBlb2YgeyBleGl0IDAgfQogICAgdGltZW91dCB7IHB1dHMgIlRpbWVvdXQgd2FpdGluZyBmb3IgY29tcGxldGlvbiI7IGV4aXQgMSB9Cn0K"

          EXPECT_SCRIPT=$(mktemp)
          echo "$EXPECT_B64" | base64 -d > "$EXPECT_SCRIPT"
          chmod +x "$EXPECT_SCRIPT"

          # Run the expect script with prompt file as argument
          expect "$EXPECT_SCRIPT" "$PROMPT_FILE" 2>&1 | tee "$LOG_FILE"

          rm -f "$EXPECT_SCRIPT"

          CLAUDE_EXIT_CODE=${PIPESTATUS[0]}
          set -e

          rm -f "$PROMPT_FILE"

          echo "claude_exit_code=$CLAUDE_EXIT_CODE" >> $GITHUB_OUTPUT
          echo "============================================"
          echo "Chunk $CHUNK_INDEX - Claude CLI exit code: $CLAUDE_EXIT_CODE"
          echo "End time: $(date -u +%Y-%m-%dT%H:%M:%SZ)"
          echo "============================================"

      - name: Validate Chunk Outputs
        id: validate_outputs
        if: always()
        env:
          FOLDER: ${{ needs.prepare.outputs.folder }}
          CHUNK_INDEX: ${{ matrix.chunk.index }}
        run: |
          CHUNK_RESULT="$GITHUB_WORKSPACE/$FOLDER/validation/chunks/results/result-chunk-$CHUNK_INDEX.json"
          SCREENSHOT_DIR="$GITHUB_WORKSPACE/$FOLDER/validation/screenshots/chunk-$CHUNK_INDEX"

          STATUS="success"
          TESTS_PASSED=0
          TESTS_FAILED=0
          SCREENSHOT_COUNT=0

          # Check chunk result
          if [ -f "$CHUNK_RESULT" ]; then
            if jq empty "$CHUNK_RESULT" 2>/dev/null; then
              TESTS_PASSED=$(jq -r '.summary.passed // 0' "$CHUNK_RESULT")
              TESTS_FAILED=$(jq -r '.summary.failed // 0' "$CHUNK_RESULT")
              VALIDATION_STATUS=$(jq -r '.validation_status // "unknown"' "$CHUNK_RESULT")
              if [ "$VALIDATION_STATUS" = "failed" ]; then
                STATUS="failed"
              fi
            else
              STATUS="failed"
            fi
          else
            STATUS="failed"
          fi

          # Count screenshots
          if [ -d "$SCREENSHOT_DIR" ]; then
            SCREENSHOT_COUNT=$(find "$SCREENSHOT_DIR" -name "*.png" | wc -l | tr -d ' ')
          fi

          echo "status=$STATUS" >> $GITHUB_OUTPUT
          echo "tests_passed=$TESTS_PASSED" >> $GITHUB_OUTPUT
          echo "tests_failed=$TESTS_FAILED" >> $GITHUB_OUTPUT
          echo "screenshot_count=$SCREENSHOT_COUNT" >> $GITHUB_OUTPUT

          echo "Chunk $CHUNK_INDEX: Status=$STATUS, Passed=$TESTS_PASSED, Failed=$TESTS_FAILED, Screenshots=$SCREENSHOT_COUNT"

      - name: Cleanup Browser Processes
        if: always()
        run: |
          # Only kill Playwright processes, NOT user's personal browsers
          pkill -f "mcp-server-playwright" 2>/dev/null || true
          pkill -f "playwright.*chromium" 2>/dev/null || true
          pkill -f "ms-playwright" 2>/dev/null || true
          echo "Playwright cleanup complete"

      - name: Upload Chunk Artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: chunk-${{ matrix.chunk.index }}-results
          path: |
            ${{ needs.prepare.outputs.folder }}/validation/chunks/results/result-chunk-${{ matrix.chunk.index }}.json
            ${{ needs.prepare.outputs.folder }}/validation/screenshots/chunk-${{ matrix.chunk.index }}/
            ${{ needs.prepare.outputs.folder }}/validation/chunks/validation-chunk-${{ matrix.chunk.index }}.log
          retention-days: 7

  # ============================================
  # JOB 3: MERGE - Combine chunk results and commit
  # ============================================
  merge-results:
    needs: [prepare, validate]
    if: always() && needs.prepare.outputs.skip != 'true' && needs.prepare.outputs.chunk_count != '0'
    runs-on: [self-hosted, validation]
    timeout-minutes: 10

    outputs:
      validation_status: ${{ steps.merge.outputs.status }}
      tests_passed: ${{ steps.merge.outputs.tests_passed }}
      tests_failed: ${{ steps.merge.outputs.tests_failed }}
      screenshot_count: ${{ steps.merge.outputs.screenshot_count }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        timeout-minutes: 3
        with:
          fetch-depth: 1
          persist-credentials: false

      - name: Download All Chunk Artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: chunk-*-results
          path: ${{ needs.prepare.outputs.folder }}/validation/artifacts/

      - name: Merge Chunk Results
        id: merge
        env:
          FOLDER: ${{ needs.prepare.outputs.folder }}
          CHUNK_COUNT: ${{ needs.prepare.outputs.chunk_count }}
          FEATURE_NAME: ${{ needs.prepare.outputs.feature_name }}
          FEATURE_SLUG: ${{ needs.prepare.outputs.feature_slug }}
          VERSION: ${{ needs.prepare.outputs.version }}
          TOTAL_TESTS: ${{ needs.prepare.outputs.total_tests }}
        run: |
          RESULTS_DIR="$GITHUB_WORKSPACE/$FOLDER/validation/chunks/results"
          ARTIFACTS_DIR="$GITHUB_WORKSPACE/$FOLDER/validation/artifacts"
          FINAL_RESULT="$GITHUB_WORKSPACE/$FOLDER/validation/result.json"
          SCREENSHOTS_DIR="$GITHUB_WORKSPACE/$FOLDER/validation/screenshots"

          mkdir -p "$RESULTS_DIR"
          mkdir -p "$SCREENSHOTS_DIR"

          echo "============================================"
          echo "Merging results from $CHUNK_COUNT chunks"
          echo "============================================"

          # Copy chunk results from artifacts
          # Note: upload-artifact strips common prefix, so internal paths are relative
          for i in $(seq 0 $((CHUNK_COUNT - 1))); do
            # Try both possible paths (with and without $FOLDER prefix)
            ARTIFACT_RESULT_1="$ARTIFACTS_DIR/chunk-$i-results/chunks/results/result-chunk-$i.json"
            ARTIFACT_RESULT_2="$ARTIFACTS_DIR/chunk-$i-results/$FOLDER/validation/chunks/results/result-chunk-$i.json"

            if [ -f "$ARTIFACT_RESULT_1" ]; then
              cp "$ARTIFACT_RESULT_1" "$RESULTS_DIR/"
              echo "Copied chunk-$i result (path type 1)"
            elif [ -f "$ARTIFACT_RESULT_2" ]; then
              cp "$ARTIFACT_RESULT_2" "$RESULTS_DIR/"
              echo "Copied chunk-$i result (path type 2)"
            else
              echo "WARNING: Could not find chunk-$i result at either path"
              echo "  Tried: $ARTIFACT_RESULT_1"
              echo "  Tried: $ARTIFACT_RESULT_2"
              ls -la "$ARTIFACTS_DIR/chunk-$i-results/" 2>/dev/null || true
            fi

            # Copy screenshots - try both paths
            ARTIFACT_SCREENSHOTS_1="$ARTIFACTS_DIR/chunk-$i-results/screenshots/chunk-$i"
            ARTIFACT_SCREENSHOTS_2="$ARTIFACTS_DIR/chunk-$i-results/$FOLDER/validation/screenshots/chunk-$i"

            if [ -d "$ARTIFACT_SCREENSHOTS_1" ]; then
              cp -r "$ARTIFACT_SCREENSHOTS_1"/* "$SCREENSHOTS_DIR/" 2>/dev/null || true
              echo "Copied chunk-$i screenshots (path type 1)"
            elif [ -d "$ARTIFACT_SCREENSHOTS_2" ]; then
              cp -r "$ARTIFACT_SCREENSHOTS_2"/* "$SCREENSHOTS_DIR/" 2>/dev/null || true
              echo "Copied chunk-$i screenshots (path type 2)"
            fi
          done

          # Merge all chunk results into final result.json
          TOTAL_PASSED=0
          TOTAL_FAILED=0
          TOTAL_SKIPPED=0
          ALL_TEST_RESULTS="[]"
          ALL_SCREENSHOTS="[]"
          OVERALL_STATUS="completed"

          for i in $(seq 0 $((CHUNK_COUNT - 1))); do
            CHUNK_RESULT="$RESULTS_DIR/result-chunk-$i.json"
            if [ -f "$CHUNK_RESULT" ] && jq empty "$CHUNK_RESULT" 2>/dev/null; then
              PASSED=$(jq -r '.summary.passed // 0' "$CHUNK_RESULT")
              FAILED=$(jq -r '.summary.failed // 0' "$CHUNK_RESULT")
              SKIPPED=$(jq -r '.summary.skipped // 0' "$CHUNK_RESULT")
              STATUS=$(jq -r '.validation_status // "unknown"' "$CHUNK_RESULT")

              TOTAL_PASSED=$((TOTAL_PASSED + PASSED))
              TOTAL_FAILED=$((TOTAL_FAILED + FAILED))
              TOTAL_SKIPPED=$((TOTAL_SKIPPED + SKIPPED))

              if [ "$STATUS" = "failed" ]; then
                OVERALL_STATUS="failed"
              fi

              # Merge test results
              ALL_TEST_RESULTS=$(echo "$ALL_TEST_RESULTS" | jq --slurpfile chunk "$CHUNK_RESULT" '. + ($chunk[0].test_results // [])')
              ALL_SCREENSHOTS=$(echo "$ALL_SCREENSHOTS" | jq --slurpfile chunk "$CHUNK_RESULT" '. + ($chunk[0].screenshots // [])')

              echo "Chunk $i: $PASSED passed, $FAILED failed, $SKIPPED skipped"
            else
              echo "Chunk $i: Result missing or invalid"
              OVERALL_STATUS="failed"
            fi
          done

          # Count total screenshots
          SCREENSHOT_COUNT=$(find "$SCREENSHOTS_DIR" -name "*.png" 2>/dev/null | wc -l | tr -d ' ')

          # Write final merged result
          jq -n \
            --arg status "$OVERALL_STATUS" \
            --arg feature_name "$FEATURE_NAME" \
            --arg feature_slug "$FEATURE_SLUG" \
            --arg version "$VERSION" \
            --arg timestamp "$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
            --argjson total_tests "$TOTAL_TESTS" \
            --argjson passed "$TOTAL_PASSED" \
            --argjson failed "$TOTAL_FAILED" \
            --argjson skipped "$TOTAL_SKIPPED" \
            --argjson chunk_count "$CHUNK_COUNT" \
            --argjson test_results "$ALL_TEST_RESULTS" \
            --argjson screenshots "$ALL_SCREENSHOTS" \
            '{
              validation_status: $status,
              feature_name: $feature_name,
              feature_slug: $feature_slug,
              version: $version,
              timestamp: $timestamp,
              execution_mode: "chunked",
              chunk_count: $chunk_count,
              summary: {
                total_tests: $total_tests,
                passed: $passed,
                failed: $failed,
                skipped: $skipped
              },
              test_results: $test_results,
              screenshots: $screenshots
            }' > "$FINAL_RESULT"

          echo "status=$OVERALL_STATUS" >> $GITHUB_OUTPUT
          echo "tests_passed=$TOTAL_PASSED" >> $GITHUB_OUTPUT
          echo "tests_failed=$TOTAL_FAILED" >> $GITHUB_OUTPUT
          echo "screenshot_count=$SCREENSHOT_COUNT" >> $GITHUB_OUTPUT

          echo "============================================"
          echo "Merged Results Summary:"
          echo "  Status: $OVERALL_STATUS"
          echo "  Total Passed: $TOTAL_PASSED"
          echo "  Total Failed: $TOTAL_FAILED"
          echo "  Total Skipped: $TOTAL_SKIPPED"
          echo "  Screenshots: $SCREENSHOT_COUNT"
          echo "============================================"

      - name: Generate Report
        env:
          FOLDER: ${{ needs.prepare.outputs.folder }}
          FEATURE_NAME: ${{ needs.prepare.outputs.feature_name }}
        run: |
          RESULT_FILE="$GITHUB_WORKSPACE/$FOLDER/validation/result.json"
          REPORT_FILE="$GITHUB_WORKSPACE/$FOLDER/validation/report.md"

          if [ -f "$RESULT_FILE" ]; then
            PASSED=$(jq -r '.summary.passed // 0' "$RESULT_FILE")
            FAILED=$(jq -r '.summary.failed // 0' "$RESULT_FILE")
            SKIPPED=$(jq -r '.summary.skipped // 0' "$RESULT_FILE")
            STATUS=$(jq -r '.validation_status // "unknown"' "$RESULT_FILE")
            CHUNKS=$(jq -r '.chunk_count // 1' "$RESULT_FILE")

            cat > "$REPORT_FILE" << EOF
          # Validation Report: $FEATURE_NAME

          **Status**: $STATUS
          **Execution Mode**: Chunked ($CHUNKS chunks)
          **Generated**: $(date -u +%Y-%m-%dT%H:%M:%SZ)

          ## Summary

          | Metric | Count |
          |--------|-------|
          | Passed | $PASSED |
          | Failed | $FAILED |
          | Skipped | $SKIPPED |
          | Total | $((PASSED + FAILED + SKIPPED)) |

          ## Test Results

          EOF

            # Add individual test results
            jq -r '.test_results[] | "### \(.test_name)\n- **Status**: \(.status)\n- **Assertions**: \(.assertions_passed // 0)/\(.assertions_checked // 0)\n- **Notes**: \(.notes // "None")\n"' "$RESULT_FILE" >> "$REPORT_FILE" 2>/dev/null || true

            echo "Report generated: $REPORT_FILE"
          fi

      - name: Commit Results
        env:
          PAT_TOKEN: ${{ secrets.PAT_TOKEN }}
          FOLDER: ${{ needs.prepare.outputs.folder }}
        run: |
          git config user.name "GitHub Actions"
          git config user.email "actions@github.com"

          git remote set-url origin "https://x-access-token:${PAT_TOKEN}@github.com/${{ github.repository }}"

          git add "$FOLDER/validation/" || true

          STATUS="${{ steps.merge.outputs.status }}"
          PASSED="${{ steps.merge.outputs.tests_passed }}"
          FAILED="${{ steps.merge.outputs.tests_failed }}"
          CHUNKS="${{ needs.prepare.outputs.chunk_count }}"

          if [ "$STATUS" = "completed" ]; then
            COMMIT_MSG="validation: ${{ needs.prepare.outputs.feature_name }} - ${PASSED} passed, ${FAILED} failed (${CHUNKS} chunks)"
          else
            COMMIT_MSG="validation: FAILED for ${{ needs.prepare.outputs.feature_name }} (${CHUNKS} chunks)"
          fi

          git commit -m "$COMMIT_MSG" || echo "No changes to commit"
          git push || echo "Push failed"

      - name: Notify n8n Complete
        if: always()
        run: |
          curl -s -X POST "https://automation-wh.bayzat.com/webhook/validation-complete" \
            -H "Content-Type: application/json" \
            -d "{
              \"feature_folder\": \"${{ needs.prepare.outputs.folder }}\",
              \"feature_name\": \"${{ needs.prepare.outputs.feature_name }}\",
              \"feature_slug\": \"${{ needs.prepare.outputs.feature_slug }}\",
              \"version\": \"${{ needs.prepare.outputs.version }}\",
              \"status\": \"${{ steps.merge.outputs.status }}\",
              \"tests_passed\": ${{ steps.merge.outputs.tests_passed || 0 }},
              \"tests_failed\": ${{ steps.merge.outputs.tests_failed || 0 }},
              \"screenshot_count\": ${{ steps.merge.outputs.screenshot_count || 0 }},
              \"total_tests\": ${{ needs.prepare.outputs.total_tests || 0 }},
              \"must_test\": ${{ needs.prepare.outputs.must_test || 0 }},
              \"should_test\": ${{ needs.prepare.outputs.should_test || 0 }},
              \"chunk_count\": ${{ needs.prepare.outputs.chunk_count || 1 }},
              \"execution_mode\": \"chunked\",
              \"run_id\": \"${{ github.run_id }}\",
              \"run_url\": \"https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}\",
              \"timestamp\": \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\",
              \"playwright_mcp_version\": \"${PLAYWRIGHT_MCP_VERSION}\"
            }" || echo "Webhook failed (non-blocking)"
