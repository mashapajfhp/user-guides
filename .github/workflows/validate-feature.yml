name: Validate Feature Guide

on:
  push:
    paths:
      - '**/validation/validation-payload.json'
      - '**/validation/journeys-payload.json'
  workflow_dispatch:
    inputs:
      feature_folder:
        description: 'Feature folder to validate (e.g., air-ticket/v19)'
        required: true
      debug_mode:
        description: 'Enable debug logging'
        required: false
        default: 'false'

env:
  PLAYWRIGHT_MCP_VERSION: "0.0.42"
  NODE_VERSION: "20"

jobs:
  validate:
    runs-on: [self-hosted, validation]
    timeout-minutes: 45

    outputs:
      validation_status: ${{ steps.validate_outputs.outputs.status }}
      tests_passed: ${{ steps.validate_outputs.outputs.tests_passed }}
      tests_failed: ${{ steps.validate_outputs.outputs.tests_failed }}
      screenshot_count: ${{ steps.validate_outputs.outputs.screenshot_count }}
      error_message: ${{ steps.validate_outputs.outputs.error_message }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        timeout-minutes: 3
        with:
          fetch-depth: 2
          persist-credentials: false

      - name: Detect validation request
        id: detect
        run: |
          if [ -n "${{ github.event.inputs.feature_folder }}" ]; then
            FOLDER="${{ github.event.inputs.feature_folder }}"
          else
            CHANGED_FILE=$(git diff --name-only HEAD~1 HEAD | grep -E 'validation/(validation-payload|journeys-payload)\.json' | head -1)
            if [ -n "$CHANGED_FILE" ]; then
              FOLDER=$(echo "$CHANGED_FILE" | sed 's|/validation/.*||')
              echo "Detected changed file: $CHANGED_FILE"
              echo "Extracted folder: $FOLDER"
            fi
          fi

          if [ -z "$FOLDER" ]; then
            echo "No validation request found"
            echo "skip=true" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Find payload file
          if [ -f "$GITHUB_WORKSPACE/$FOLDER/validation/journeys-payload.json" ]; then
            PAYLOAD_FILE="$FOLDER/validation/journeys-payload.json"
            PAYLOAD_FORMAT="journeys-v1"
          elif [ -f "$GITHUB_WORKSPACE/$FOLDER/validation/validation-payload.json" ]; then
            PAYLOAD_FILE="$FOLDER/validation/validation-payload.json"
            PAYLOAD_FORMAT="journeys-v1"
          else
            echo "No validation payload found in $FOLDER/validation/"
            echo "skip=true" >> $GITHUB_OUTPUT
            exit 0
          fi

          echo "folder=$FOLDER" >> $GITHUB_OUTPUT
          echo "payload_file=$PAYLOAD_FILE" >> $GITHUB_OUTPUT
          echo "payload_format=$PAYLOAD_FORMAT" >> $GITHUB_OUTPUT
          echo "result_file=$FOLDER/validation/result.json" >> $GITHUB_OUTPUT
          echo "report_file=$FOLDER/validation/report.md" >> $GITHUB_OUTPUT
          echo "screenshots_dir=$FOLDER/validation/screenshots" >> $GITHUB_OUTPUT
          echo "log_file=$FOLDER/validation/validation.log" >> $GITHUB_OUTPUT
          echo "skip=false" >> $GITHUB_OUTPUT
          echo "Found validation request: $FOLDER (format: $PAYLOAD_FORMAT)"

      - name: Parse and validate payload
        id: parse_payload
        if: steps.detect.outputs.skip != 'true'
        run: |
          PAYLOAD_PATH="$GITHUB_WORKSPACE/${{ steps.detect.outputs.payload_file }}"

          echo "Parsing payload: $PAYLOAD_PATH"

          # Validate JSON
          if ! jq empty "$PAYLOAD_PATH" 2>/dev/null; then
            echo "Invalid JSON in payload file"
            exit 1
          fi

          # Extract metadata (handle array or object)
          IS_ARRAY=$(jq 'if type == "array" then true else false end' "$PAYLOAD_PATH")

          if [ "$IS_ARRAY" = "true" ]; then
            FEATURE_NAME=$(jq -r '.[0].metadata.feature_name // "unknown"' "$PAYLOAD_PATH")
            FEATURE_SLUG=$(jq -r '.[0].metadata.feature_slug // "unknown"' "$PAYLOAD_PATH")
            VERSION=$(jq -r '.[0].metadata.version // "v1"' "$PAYLOAD_PATH")
            TOTAL_TESTS=$(jq -r '.[0].summary.total_tests // 0' "$PAYLOAD_PATH")
            MUST_TEST=$(jq -r '.[0].summary.by_priority.must_test // 0' "$PAYLOAD_PATH")
            SHOULD_TEST=$(jq -r '.[0].summary.by_priority.should_test // 0' "$PAYLOAD_PATH")
            SETUP_FLOWS_COUNT=$(jq -r '.[0].setup_flows | length' "$PAYLOAD_PATH")
            TESTS_COUNT=$(jq -r '.[0].tests | length' "$PAYLOAD_PATH")
          else
            FEATURE_NAME=$(jq -r '.metadata.feature_name // "unknown"' "$PAYLOAD_PATH")
            FEATURE_SLUG=$(jq -r '.metadata.feature_slug // "unknown"' "$PAYLOAD_PATH")
            VERSION=$(jq -r '.metadata.version // "v1"' "$PAYLOAD_PATH")
            TOTAL_TESTS=$(jq -r '.summary.total_tests // 0' "$PAYLOAD_PATH")
            MUST_TEST=$(jq -r '.summary.by_priority.must_test // 0' "$PAYLOAD_PATH")
            SHOULD_TEST=$(jq -r '.summary.by_priority.should_test // 0' "$PAYLOAD_PATH")
            SETUP_FLOWS_COUNT=$(jq -r '.setup_flows | length' "$PAYLOAD_PATH")
            TESTS_COUNT=$(jq -r '.tests | length' "$PAYLOAD_PATH")
          fi

          echo "feature_name=$FEATURE_NAME" >> $GITHUB_OUTPUT
          echo "feature_slug=$FEATURE_SLUG" >> $GITHUB_OUTPUT
          echo "version=$VERSION" >> $GITHUB_OUTPUT
          echo "total_tests=$TOTAL_TESTS" >> $GITHUB_OUTPUT
          echo "must_test=$MUST_TEST" >> $GITHUB_OUTPUT
          echo "should_test=$SHOULD_TEST" >> $GITHUB_OUTPUT
          echo "setup_flows_count=$SETUP_FLOWS_COUNT" >> $GITHUB_OUTPUT
          echo "tests_count=$TESTS_COUNT" >> $GITHUB_OUTPUT

          echo "============================================"
          echo "Payload Summary:"
          echo "  Feature: $FEATURE_NAME ($FEATURE_SLUG)"
          echo "  Version: $VERSION"
          echo "  Total Tests: $TOTAL_TESTS"
          echo "  Must Test: $MUST_TEST"
          echo "  Should Test: $SHOULD_TEST"
          echo "  Setup Flows: $SETUP_FLOWS_COUNT"
          echo "  Test Cases: $TESTS_COUNT"
          echo "============================================"

      - name: Pre-flight Health Checks
        id: preflight
        if: steps.detect.outputs.skip != 'true'
        run: |
          echo "Running pre-flight checks..."
          ERRORS=""

          if [ -z "${{ secrets.ANTHROPIC_API_KEY }}" ]; then
            ERRORS="${ERRORS}ANTHROPIC_API_KEY not configured\n"
          else
            echo "ANTHROPIC_API_KEY configured"
          fi

          if [ -z "${{ secrets.APP_BASE_URL }}" ]; then
            ERRORS="${ERRORS}APP_BASE_URL not configured\n"
          else
            echo "APP_BASE_URL configured"
          fi

          AVAILABLE_GB=$(df -g "$GITHUB_WORKSPACE" | tail -1 | awk '{print $4}')
          if [ "$AVAILABLE_GB" -lt 2 ]; then
            ERRORS="${ERRORS}Low disk space: ${AVAILABLE_GB}GB available (need 2GB)\n"
          else
            echo "Disk space OK: ${AVAILABLE_GB}GB available"
          fi

          if [ -n "$ERRORS" ]; then
            echo "Pre-flight checks failed:"
            echo -e "$ERRORS"
            exit 1
          fi

          echo "All pre-flight checks passed"

      - name: Prepare Output Directories
        if: steps.detect.outputs.skip != 'true'
        run: |
          mkdir -p "$GITHUB_WORKSPACE/${{ steps.detect.outputs.screenshots_dir }}"
          echo "Screenshots directory ready"

      - name: Install Claude CLI
        if: steps.detect.outputs.skip != 'true'
        run: |
          echo "Installing Claude CLI..."
          npm install -g @anthropic-ai/claude-code
          export PATH="$(npm prefix -g)/bin:$PATH"

          if ! command -v claude &> /dev/null; then
            echo "Claude CLI installation failed"
            exit 1
          fi

          claude --version
          echo "Claude CLI installed"

      - name: Install Playwright MCP and Browsers
        if: steps.detect.outputs.skip != 'true'
        run: |
          echo "Installing Playwright MCP v${PLAYWRIGHT_MCP_VERSION}..."
          npm install -g @playwright/mcp@${PLAYWRIGHT_MCP_VERSION}

          echo "Verifying Chrome browser installation..."
          # Chrome is pre-installed on this runner - just verify it exists
          if [ -d ~/.cache/ms-playwright/chrome* ] || [ -d ~/.cache/ms-playwright/chromium* ]; then
            echo "Browser found in cache"
          else
            echo "Installing Chrome browser..."
            npx -y playwright install chrome
          fi

          # List installed browsers
          echo "Installed browsers:"
          ls -la ~/.cache/ms-playwright/ 2>/dev/null || echo "Cache directory empty"

          echo "Playwright MCP v${PLAYWRIGHT_MCP_VERSION} and Chrome browser ready"

      - name: Setup MCP Configuration
        if: steps.detect.outputs.skip != 'true'
        run: |
          echo "Setting up MCP configuration..."
          rm -f "$GITHUB_WORKSPACE/.mcp.json"

          SCREENSHOT_PATH="$GITHUB_WORKSPACE/${{ steps.detect.outputs.screenshots_dir }}"

          # Use Chrome browser (pre-installed on runner)
          cat > "$GITHUB_WORKSPACE/.mcp.json" << EOF
          {
            "mcpServers": {
              "playwright": {
                "command": "npx",
                "args": [
                  "-y",
                  "@playwright/mcp@${PLAYWRIGHT_MCP_VERSION}",
                  "--output-dir", "${SCREENSHOT_PATH}",
                  "--browser", "chrome",
                  "--timeout", "300000"
                ]
              }
            }
          }
          EOF

          echo "MCP config:"
          cat "$GITHUB_WORKSPACE/.mcp.json"
          echo "MCP configuration ready"

      - name: Generate Validation Prompt
        id: generate_prompt
        if: steps.detect.outputs.skip != 'true'
        env:
          APP_BASE_URL: ${{ secrets.APP_BASE_URL }}
          APP_USERNAME: ${{ secrets.APP_USERNAME }}
          APP_PASSWORD: ${{ secrets.APP_PASSWORD }}
          FEATURE_FOLDER: ${{ steps.detect.outputs.folder }}
          PAYLOAD_FILE: ${{ steps.detect.outputs.payload_file }}
          SCREENSHOTS_DIR: ${{ steps.detect.outputs.screenshots_dir }}
          RESULT_FILE: ${{ steps.detect.outputs.result_file }}
          FEATURE_NAME: ${{ steps.parse_payload.outputs.feature_name }}
        run: |
          PROMPT_FILE="$GITHUB_WORKSPACE/$FEATURE_FOLDER/validation/.prompt.txt"
          PAYLOAD_PATH="$GITHUB_WORKSPACE/$PAYLOAD_FILE"

          cat > "$PROMPT_FILE" << 'PROMPT_HEADER'
          # UI Validation Agent - Journey-Based Testing

          You are a UI validation agent that executes test cases against a live web application.
          Your task is to validate the UI based on the provided test payload.

          ## CRITICAL INSTRUCTIONS
          1. You MUST complete all validation tasks
          2. You MUST capture screenshots for each major step
          3. You MUST write result.json before finishing
          4. You MUST handle login first before any validation
          5. Focus on MUST_TEST priority items first, then SHOULD_TEST

          ═══════════════════════════════════════════════════════════════
          ## CRITICAL: REFERENCE-BASED PAYLOAD STRUCTURE
          ═══════════════════════════════════════════════════════════════

          This payload uses ID REFERENCES to avoid duplication. You MUST resolve
          references before executing any test. The payload contains an "instructions"
          section that also explains this - read it carefully.

          ### REFERENCE RESOLUTION TABLE
          ┌─────────────────────┬──────────────────────────────────────────────┐
          │ Field               │ How to Resolve                               │
          ├─────────────────────┼──────────────────────────────────────────────┤
          │ access_path_ref     │ Look up in definitions.access_paths          │
          │                     │ (configuration OR operational) by path_id    │
          ├─────────────────────┼──────────────────────────────────────────────┤
          │ setup_flow_ref      │ Look up in definitions.setup_flows           │
          │                     │ by flow_id → get steps array                 │
          ├─────────────────────┼──────────────────────────────────────────────┤
          │ business_rule_refs  │ Look up each ID in definitions.business_rules│
          │                     │ by rule_id → get rule + ui_manifestation     │
          ├─────────────────────┼──────────────────────────────────────────────┤
          │ edge_case_refs      │ Look up each ID in definitions.edge_cases    │
          │                     │ by case_id → get scenario + expected_behavior│
          ├─────────────────────┼──────────────────────────────────────────────┤
          │ ui_element_refs     │ Look up each in definitions.ui_elements      │
          │                     │ by element_name → get type + location        │
          └─────────────────────┴──────────────────────────────────────────────┘

          ### RESOLUTION EXAMPLE
          Given test with: setup_flow_ref: "SETUP-002"
          Action: Find definitions.setup_flows.find(f => f.flow_id === "SETUP-002")
          Result: Execute all steps from that flow object

          Given test with: business_rule_refs: ["BR-001", "BR-005"]
          Action: Find each in definitions.business_rules by rule_id
          Result: Validate UI behaves according to those rules

          ═══════════════════════════════════════════════════════════════

          PROMPT_HEADER

          cat >> "$PROMPT_FILE" << EOF

          ## AUTHENTICATION
          - Base URL: $APP_BASE_URL
          - Username: $APP_USERNAME
          - Password: $APP_PASSWORD

          ## OUTPUT PATHS
          - Screenshots: $GITHUB_WORKSPACE/$SCREENSHOTS_DIR/
          - Result JSON: $GITHUB_WORKSPACE/$RESULT_FILE
          - Report: $GITHUB_WORKSPACE/$FEATURE_FOLDER/validation/report.md

          ## FEATURE: $FEATURE_NAME

          ## VALIDATION PAYLOAD
          Read and parse the following payload file:
          $GITHUB_WORKSPACE/$PAYLOAD_FILE

          ## VALIDATION WORKFLOW

          ### Step 1: Login
          1. Navigate to the base URL
          2. Enter credentials and login
          3. Take screenshot: "01-login-success.png"

          ### Step 2: Resolve References and Execute Setup Flows
          For each test that has a setup_flow_ref:
          1. RESOLVE: Look up the flow_id in definitions.setup_flows
          2. RESOLVE: Look up access_path_ref in definitions.access_paths (if present)
          3. Follow the navigation path from the resolved access_path
          4. Execute each step from the resolved setup_flow
          5. Take screenshot after key interactions
          6. Record success/failure for each step

          ### Step 3: Execute Test Cases
          For each test in the payload (prioritize must_test first):
          1. RESOLVE access_path_ref → get navigation steps
          2. RESOLVE setup_flow_ref → get setup steps
          3. RESOLVE business_rule_refs → get rules to validate
          4. RESOLVE edge_case_refs → get edge cases to check
          5. Navigate using the resolved navigation path
          6. Execute the resolved setup_flow steps
          7. Validate each assertion in the test
          8. Check resolved business_rules are satisfied
          9. Test resolved edge_cases if applicable
          10. Take screenshots showing validation results

          ### Step 4: Write Results
          Write a result.json file with this structure:
          {
            "validation_status": "completed|failed",
            "feature_name": "$FEATURE_NAME",
            "timestamp": "<ISO timestamp>",
            "summary": {
              "total_tests": <number>,
              "passed": <number>,
              "failed": <number>,
              "skipped": <number>
            },
            "test_results": [
              {
                "test_id": "<test_id from payload>",
                "test_name": "<test name>",
                "status": "passed|failed|skipped",
                "assertions_checked": <number>,
                "assertions_passed": <number>,
                "business_rules_validated": ["<rule_ids checked>"],
                "edge_cases_tested": ["<case_ids checked>"],
                "screenshots": ["<filename>"],
                "notes": "<any observations>"
              }
            ],
            "setup_flow_results": [
              {
                "flow_id": "<flow_id>",
                "flow_name": "<name>",
                "status": "completed|failed",
                "steps_completed": <number>,
                "total_steps": <number>
              }
            ],
            "screenshots": ["<list of all screenshots>"]
          }

          ## IMPORTANT NOTES
          - ALWAYS resolve references before executing - never skip this step
          - Use element_type hints from the resolved steps (button, input, dropdown, etc.)
          - Verify expected_result for each resolved step
          - Handle modal dialogs and page transitions gracefully
          - If a test cannot be executed, mark as skipped with reason
          - Take screenshot on any error for debugging
          - If a reference cannot be resolved (null or not found), log it and skip that part

          EOF

          echo "Validation prompt generated ($(wc -c < "$PROMPT_FILE") bytes)"

      - name: Pre-Validation Browser Cleanup
        if: steps.detect.outputs.skip != 'true'
        run: |
          echo "Cleaning up stale browser processes..."
          pkill -f "mcp-server-playwright" 2>/dev/null || true
          pkill -f "chrome.*playwright" 2>/dev/null || true
          pkill -f "Google Chrome" 2>/dev/null || true
          pkill -f "chromium" 2>/dev/null || true
          sleep 2
          rm -rf /tmp/playwright-* 2>/dev/null || true
          echo "Pre-validation cleanup complete"

      - name: Run Playwright MCP Validation
        id: validation
        if: steps.detect.outputs.skip != 'true'
        env:
          FEATURE_FOLDER: ${{ steps.detect.outputs.folder }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          echo "============================================"
          echo "Starting validation for: $FEATURE_FOLDER"
          echo "Feature: ${{ steps.parse_payload.outputs.feature_name }}"
          echo "Tests to run: ${{ steps.parse_payload.outputs.total_tests }}"
          echo "Start time: $(date -u +%Y-%m-%dT%H:%M:%SZ)"
          echo "============================================"

          cd "$GITHUB_WORKSPACE"
          export PATH="$(npm prefix -g)/bin:$PATH"

          PROMPT_FILE="$GITHUB_WORKSPACE/$FEATURE_FOLDER/validation/.prompt.txt"

          set +e
          claude --model sonnet \
                 --allowedTools "mcp__playwright__*,Read,Write,Glob,Grep" \
                 --mcp-config ".mcp.json" \
                 --dangerously-skip-permissions \
                 --print "$(cat "$PROMPT_FILE")" 2>&1 | tee "$GITHUB_WORKSPACE/${{ steps.detect.outputs.log_file }}"

          CLAUDE_EXIT_CODE=$?
          set -e

          rm -f "$PROMPT_FILE"

          echo "claude_exit_code=$CLAUDE_EXIT_CODE" >> $GITHUB_OUTPUT
          echo "============================================"
          echo "Claude CLI exit code: $CLAUDE_EXIT_CODE"
          echo "End time: $(date -u +%Y-%m-%dT%H:%M:%SZ)"
          echo "============================================"

      - name: Validate Outputs
        id: validate_outputs
        if: always() && steps.detect.outputs.skip != 'true'
        run: |
          echo "Validating output files..."

          FOLDER="${{ steps.detect.outputs.folder }}"
          SCREENSHOTS_DIR="${{ steps.detect.outputs.screenshots_dir }}"
          RESULT_FILE="${{ steps.detect.outputs.result_file }}"
          LOG_FILE="${{ steps.detect.outputs.log_file }}"

          STATUS="success"
          ERROR_MESSAGE=""
          SCREENSHOT_COUNT=0
          TESTS_PASSED=0
          TESTS_FAILED=0

          # Check validation log
          LOG_SIZE=$(wc -c < "$GITHUB_WORKSPACE/$LOG_FILE" 2>/dev/null || echo "0")
          echo "Validation log size: $LOG_SIZE bytes"
          if [ "$LOG_SIZE" -lt 100 ]; then
            STATUS="failed"
            ERROR_MESSAGE="Validation log is empty or too small"
          fi

          # Check screenshots
          if [ -d "$GITHUB_WORKSPACE/$SCREENSHOTS_DIR" ]; then
            SCREENSHOT_COUNT=$(find "$GITHUB_WORKSPACE/$SCREENSHOTS_DIR" -name "*.png" | wc -l | tr -d ' ')
            echo "Screenshots found: $SCREENSHOT_COUNT"
          fi

          # Check and parse result.json
          if [ -f "$GITHUB_WORKSPACE/$RESULT_FILE" ]; then
            if jq empty "$GITHUB_WORKSPACE/$RESULT_FILE" 2>/dev/null; then
              echo "result.json is valid JSON"
              TESTS_PASSED=$(jq -r '.summary.passed // 0' "$GITHUB_WORKSPACE/$RESULT_FILE")
              TESTS_FAILED=$(jq -r '.summary.failed // 0' "$GITHUB_WORKSPACE/$RESULT_FILE")
              VALIDATION_STATUS=$(jq -r '.validation_status // "unknown"' "$GITHUB_WORKSPACE/$RESULT_FILE")
              if [ "$VALIDATION_STATUS" = "failed" ]; then
                STATUS="failed"
              fi
            else
              STATUS="failed"
              ERROR_MESSAGE="${ERROR_MESSAGE}; result.json is not valid JSON"
            fi
          else
            STATUS="failed"
            ERROR_MESSAGE="${ERROR_MESSAGE}; result.json not generated"
          fi

          echo "status=$STATUS" >> $GITHUB_OUTPUT
          echo "tests_passed=$TESTS_PASSED" >> $GITHUB_OUTPUT
          echo "tests_failed=$TESTS_FAILED" >> $GITHUB_OUTPUT
          echo "screenshot_count=$SCREENSHOT_COUNT" >> $GITHUB_OUTPUT
          echo "error_message=$ERROR_MESSAGE" >> $GITHUB_OUTPUT

          echo "============================================"
          echo "Validation Summary"
          echo "============================================"
          echo "Status: $STATUS"
          echo "Tests Passed: $TESTS_PASSED"
          echo "Tests Failed: $TESTS_FAILED"
          echo "Screenshots: $SCREENSHOT_COUNT"
          echo "Errors: ${ERROR_MESSAGE:-none}"
          echo "============================================"

      - name: Cleanup Browser Processes
        if: always() && steps.detect.outputs.skip != 'true'
        run: |
          pkill -f "mcp-server-playwright" 2>/dev/null || true
          pkill -f "chrome.*playwright" 2>/dev/null || true
          pkill -f "Google Chrome" 2>/dev/null || true
          pkill -f "chromium" 2>/dev/null || true
          echo "Browser cleanup complete"

      - name: Commit Results
        if: steps.detect.outputs.skip != 'true'
        env:
          PAT_TOKEN: ${{ secrets.PAT_TOKEN }}
        run: |
          git config user.name "GitHub Actions"
          git config user.email "actions@github.com"

          git remote set-url origin "https://x-access-token:${PAT_TOKEN}@github.com/${{ github.repository }}"

          git add "${{ steps.detect.outputs.folder }}/validation/" || true

          STATUS="${{ steps.validate_outputs.outputs.status }}"
          PASSED="${{ steps.validate_outputs.outputs.tests_passed }}"
          FAILED="${{ steps.validate_outputs.outputs.tests_failed }}"

          if [ "$STATUS" = "success" ]; then
            COMMIT_MSG="validation: ${{ steps.parse_payload.outputs.feature_name }} - ${PASSED} passed, ${FAILED} failed"
          else
            COMMIT_MSG="validation: FAILED for ${{ steps.parse_payload.outputs.feature_name }}"
          fi

          git commit -m "$COMMIT_MSG" || echo "No changes to commit"
          git push || echo "Push failed"

      - name: Notify n8n Complete
        if: always() && steps.detect.outputs.skip != 'true'
        run: |
          curl -s -X POST "https://automation-wh.bayzat.com/webhook/validation-complete" \
            -H "Content-Type: application/json" \
            -d "{
              \"feature_folder\": \"${{ steps.detect.outputs.folder }}\",
              \"feature_name\": \"${{ steps.parse_payload.outputs.feature_name }}\",
              \"feature_slug\": \"${{ steps.parse_payload.outputs.feature_slug }}\",
              \"version\": \"${{ steps.parse_payload.outputs.version }}\",
              \"status\": \"${{ steps.validate_outputs.outputs.status }}\",
              \"tests_passed\": ${{ steps.validate_outputs.outputs.tests_passed || 0 }},
              \"tests_failed\": ${{ steps.validate_outputs.outputs.tests_failed || 0 }},
              \"screenshot_count\": ${{ steps.validate_outputs.outputs.screenshot_count || 0 }},
              \"total_tests\": ${{ steps.parse_payload.outputs.total_tests || 0 }},
              \"must_test\": ${{ steps.parse_payload.outputs.must_test || 0 }},
              \"should_test\": ${{ steps.parse_payload.outputs.should_test || 0 }},
              \"run_id\": \"${{ github.run_id }}\",
              \"run_url\": \"https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}\",
              \"timestamp\": \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\",
              \"playwright_mcp_version\": \"${PLAYWRIGHT_MCP_VERSION}\"
            }" || echo "Webhook failed (non-blocking)"
